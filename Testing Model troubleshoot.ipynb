{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\R_Lang\\\\Coursera\\\\Deep Learning\\\\Neural Network and Deep Learning\\\\Excercises\\\\Week 4\\\\Building_your_Deep_Neural_Network_Step_by_Step.tar\\\\Building_your_Deep_Neural_Network_Step_by_Step'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()\n",
    "os.chdir(\"C:\\\\R_Lang\\\\Coursera\\\\Deep Learning\\\\Neural Network and Deep Learning\\\\Excercises\\\\Week 4\\\\Building_your_Deep_Neural_Network_Step_by_Step.tar\\\\Building_your_Deep_Neural_Network_Step_by_Step\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from testCases_v3 import *\n",
    "from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "    train_dataset = h5py.File('C:\\\\R_Lang\\\\Deep_Learning_Python\\\\datasets\\\\train_catvnoncat.h5', \"r\")\n",
    "    train_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('C:\\\\R_Lang\\\\Deep_Learning_Python\\\\datasets\\\\test_catvnoncat.h5', \"r\")\n",
    "    test_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_y = train_set_y_orig.reshape(1, train_set_y_orig.shape[0]) # to covert rank 1 array to 2 d array\n",
    "    test_y = test_set_y_orig.reshape(1, test_set_y_orig.shape[0]) # to covert rank 1 array to 2 d array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 209\n",
      "Number of testing examples: 50\n",
      "Each image is of size: (64, 64, 3)\n",
      "train_x_orig shape: (209, 64, 64, 3)\n",
      "train_y shape: (1, 209)\n",
      "test_x_orig shape: (50, 64, 64, 3)\n",
      "test_y shape: (1, 50)\n"
     ]
    }
   ],
   "source": [
    "# Explore your dataset \n",
    "m_train = train_x_orig.shape[0]\n",
    "num_px = train_x_orig.shape[1]\n",
    "m_test = test_x_orig.shape[0]\n",
    "\n",
    "print (\"Number of training examples: \" + str(m_train))\n",
    "print (\"Number of testing examples: \" + str(m_test))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_x_orig shape: \" + str(train_x_orig.shape))\n",
    "print (\"train_y shape: \" + str(train_y.shape))\n",
    "print (\"test_x_orig shape: \" + str(test_x_orig.shape))\n",
    "print (\"test_y shape: \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (12288, 209)\n",
      "test_x's shape: (12288, 50)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the training and test examples \n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "\n",
    "print (\"train_x's shape: \" + str(train_x.shape))\n",
    "print (\"test_x's shape: \" + str(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### CONSTANTS ###\n",
    "layers_dims = [12288, 20, 7, 5, 1] #  5-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n",
    "\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    W1 = np.random.randn(n_h, n_x)*0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h)*0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters     \n",
    "\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters[\"W\" + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1] )*.01\n",
    "        #parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = W.dot(A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
    "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (there are (L-1) or them, indexes from 0 to L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (there is one, index L-1)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p\n",
    "\n",
    "def print_mislabeled_images(classes, X, y, p):\n",
    "    \"\"\"\n",
    "    Plots images where predictions and truth were different.\n",
    "    X -- dataset\n",
    "    y -- true labels\n",
    "    p -- predictions\n",
    "    \"\"\"\n",
    "    a = p + y\n",
    "    mislabeled_indices = np.asarray(np.where(a == 1))\n",
    "    plt.rcParams['figure.figsize'] = (40.0, 40.0) # set default size of plots\n",
    "    num_images = len(mislabeled_indices[0])\n",
    "    for i in range(num_images):\n",
    "        index = mislabeled_indices[1][i]\n",
    "        \n",
    "        plt.subplot(2, num_images, i + 1)\n",
    "        plt.imshow(X[:,index].reshape(64,64,3), interpolation='nearest')\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Prediction: \" + classes[int(p[0,index])].decode(\"utf-8\") + \" \\n Class: \" + classes[y[0,index]].decode(\"utf-8\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_layer_model\n",
    "\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    ### START CODE HERE ###\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        AL, caches = L_model_forward(X,parameters)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute cost.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "        # Backward propagation.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        ### END CODE HERE ###\n",
    " \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693149\n",
      "Cost after iteration 100: 0.678010\n",
      "Cost after iteration 200: 0.667599\n",
      "Cost after iteration 300: 0.660421\n",
      "Cost after iteration 400: 0.655457\n",
      "Cost after iteration 500: 0.652013\n",
      "Cost after iteration 600: 0.649615\n",
      "Cost after iteration 700: 0.647941\n",
      "Cost after iteration 800: 0.646769\n",
      "Cost after iteration 900: 0.645947\n",
      "Cost after iteration 1000: 0.645368\n",
      "Cost after iteration 1100: 0.644960\n",
      "Cost after iteration 1200: 0.644673\n",
      "Cost after iteration 1300: 0.644469\n",
      "Cost after iteration 1400: 0.644325\n",
      "Cost after iteration 1500: 0.644223\n",
      "Cost after iteration 1600: 0.644151\n",
      "Cost after iteration 1700: 0.644100\n",
      "Cost after iteration 1800: 0.644063\n",
      "Cost after iteration 1900: 0.644037\n",
      "Cost after iteration 2000: 0.644019\n",
      "Cost after iteration 2100: 0.644006\n",
      "Cost after iteration 2200: 0.643997\n",
      "Cost after iteration 2300: 0.643990\n",
      "Cost after iteration 2400: 0.643985\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMzPLiQPDzMxy4sAwM7Oc/H+qj9TNNb9xlQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xa412ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.655502392344\n"
     ]
    }
   ],
   "source": [
    "pred_train = predict(train_x, train_y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.34\n"
     ]
    }
   ],
   "source": [
    "pred_test = predict(test_x, test_y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
